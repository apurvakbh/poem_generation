{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Connecting with google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "YxdUFn9q8bb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a58757d-e272-4479-96e3-7236401b7010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls gdrive/MyDrive/generate_content/code\n",
        "%cd gdrive/MyDrive/generate_content/code\n",
        "\n",
        "# !mkdir data\n",
        "# !mkdir data/poetry\n",
        "# !mkdir data/poetry/trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvS9bJDnfye5",
        "outputId": "abeca4a9-4187-4366-d70d-21ee242bb42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/generate_content/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip freeze | grep tensor"
      ],
      "metadata": {
        "id": "uy_4P-ntgWCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8e8db1-9256-44d8-ced3-c040bdfbe903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytensor==2.10.1\n",
            "tensorboard==2.11.2\n",
            "tensorboard-data-server==0.6.1\n",
            "tensorboard-plugin-wit==1.8.1\n",
            "tensorflow==2.11.0\n",
            "tensorflow-datasets==4.8.3\n",
            "tensorflow-estimator==2.11.0\n",
            "tensorflow-gcs-config==2.11.0\n",
            "tensorflow-hub==0.13.0\n",
            "tensorflow-io-gcs-filesystem==0.31.0\n",
            "tensorflow-metadata==1.12.0\n",
            "tensorflow-probability==0.19.0\n",
            "tensorstore==0.1.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install tensor2tensor==1.13.1 tensorflow==1.13.1 tensorflow-serving-api==1.13# gutenberg\n",
        "pip install tensorflow_hub"
      ],
      "metadata": {
        "id": "cOvv9ofjhK5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d04e6e-a063-41f8-a527-eea43ff6d6b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensor2tensor==1.13.1\n",
            "  Downloading tensor2tensor-1.13.1-py2.py3-none-any.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 16.8 MB/s eta 0:00:00\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.9/dist-packages (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow_hub) (3.19.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow_hub) (1.22.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement tensorflow==1.13.1 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)\n",
            "ERROR: No matching distribution found for tensorflow==1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip freeze | grep tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoY3fZEw2h0H",
        "outputId": "15e0fbe3-c90a-4aab-f957-0a8d3447d035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytensor==2.10.1\n",
            "tensorboard==2.11.2\n",
            "tensorboard-data-server==0.6.1\n",
            "tensorboard-plugin-wit==1.8.1\n",
            "tensorflow==2.11.0\n",
            "tensorflow-datasets==4.8.3\n",
            "tensorflow-estimator==2.11.0\n",
            "tensorflow-gcs-config==2.11.0\n",
            "tensorflow-hub==0.13.0\n",
            "tensorflow-io-gcs-filesystem==0.31.0\n",
            "tensorflow-metadata==1.12.0\n",
            "tensorflow-probability==0.19.0\n",
            "tensorstore==0.1.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install libdb5.3-dev\n",
        "!sudo pip install gutenberg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu9m0fSY8UiM",
        "outputId": "eb0e2376-7b7b-458f-a6d5-290abe34b897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  db5.3-doc\n",
            "The following NEW packages will be installed:\n",
            "  libdb5.3-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 766 kB of archives.\n",
            "After this operation, 3,210 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libdb5.3-dev amd64 5.3.28+dfsg1-0.6ubuntu2 [766 kB]\n",
            "Fetched 766 kB in 0s (2,735 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdb5.3-dev.\n",
            "(Reading database ... 128285 files and directories currently installed.)\n",
            "Preparing to unpack .../libdb5.3-dev_5.3.28+dfsg1-0.6ubuntu2_amd64.deb ...\n",
            "Unpacking libdb5.3-dev (5.3.28+dfsg1-0.6ubuntu2) ...\n",
            "Setting up libdb5.3-dev (5.3.28+dfsg1-0.6ubuntu2) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gutenberg\n",
            "  Downloading Gutenberg-0.8.2-py3-none-any.whl (26 kB)\n",
            "Collecting bsddb3>=6.1.0\n",
            "  Downloading bsddb3-6.2.9.tar.gz (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.5/230.5 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.5.1 in /usr/local/lib/python3.9/dist-packages (from gutenberg) (2.27.1)\n",
            "Collecting rdflib-sqlalchemy>=0.3.8\n",
            "  Downloading rdflib_sqlalchemy-0.5.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.9/dist-packages (from gutenberg) (0.18.3)\n",
            "Collecting SPARQLWrapper>=1.8.2\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Collecting rdflib<5.0.0,>=4.2.0\n",
            "  Downloading rdflib-4.2.2-py3-none-any.whl (344 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.2/344.2 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from gutenberg) (67.6.0)\n",
            "Collecting pyparsing<3.0.0\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from gutenberg) (1.16.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=0.8.8\n",
            "  Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy<2.0.0,>=1.1.4 in /usr/local/lib/python3.9/dist-packages (from rdflib-sqlalchemy>=0.3.8->gutenberg) (1.4.47)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.5.1->gutenberg) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.5.1->gutenberg) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.5.1->gutenberg) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.5.1->gutenberg) (3.4)\n",
            "Collecting SPARQLWrapper>=1.8.2\n",
            "  Downloading SPARQLWrapper-1.8.5-py3-none-any.whl (26 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2.0.0,>=1.1.4->rdflib-sqlalchemy>=0.3.8->gutenberg) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (2.1.2)\n",
            "Building wheels for collected packages: bsddb3\n",
            "  Building wheel for bsddb3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bsddb3: filename=bsddb3-6.2.9-cp39-cp39-linux_x86_64.whl size=332400 sha256=babd733f2850aac4b5d23ec07506cb07f56293946551092715565f92899916e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/43/12/4fdb38046f23e3c438e45a9eabbf617eaab8c183da016b7c69\n",
            "Successfully built bsddb3\n",
            "Installing collected packages: bsddb3, pyparsing, Mako, isodate, rdflib, alembic, SPARQLWrapper, rdflib-sqlalchemy, gutenberg\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "Successfully installed Mako-1.2.4 SPARQLWrapper-1.8.5 alembic-1.10.2 bsddb3-6.2.9 gutenberg-0.8.2 isodate-0.6.1 pyparsing-2.4.7 rdflib-4.2.2 rdflib-sqlalchemy-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the data\n",
        "\n",
        "# Keep it in seperate cell only\n",
        "# Run it twice\n",
        "from gutenberg.acquire import load_etext"
      ],
      "metadata": {
        "id": "3-tIC2ELEdC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gutenberg.cleanup import strip_headers\n",
        "import re\n",
        "\n",
        "books = [\n",
        "  # bookid, skip N lines\n",
        "  (26715, 1000, 'Victorian songs'),\n",
        "  (30235, 580, 'Baldwin collection'),\n",
        "  (35402, 710, 'Swinburne collection'),\n",
        "  (574, 15, 'Blake'),\n",
        "  (1304, 172, 'Bulchevys collection'),\n",
        "  (19221, 223, 'Palgrave-Pearse collection'),\n",
        "  (15553, 522, 'Knowles collection')\n",
        "]"
      ],
      "metadata": {
        "id": "33GN1X6E2lr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/poetry/raw.txt', 'w') as ofp:\n",
        "  lineno = 0\n",
        "  for (id_nr, toskip, title) in books:\n",
        "    startline = lineno\n",
        "    text = strip_headers(load_etext(id_nr,mirror = \"http://mirrors.xmission.com/gutenberg/\")).strip()\n",
        "    lines = text.split('\\n')[toskip:]\n",
        "    # any line that is all upper case is a title or author name\n",
        "    # also don't want any lines with years (numbers)\n",
        "    for line in lines:\n",
        "      if (len(line) > 0\n",
        "          and line.upper() != line\n",
        "          and not re.match('.*[0-9]+.*', line)\n",
        "          and len(line) < 50\n",
        "         ):\n",
        "        cleaned = re.sub('[^a-z\\'\\-]+', ' ', line.strip().lower())\n",
        "        ofp.write(cleaned)\n",
        "        ofp.write('\\n')\n",
        "        lineno = lineno + 1\n",
        "      else:\n",
        "        ofp.write('\\n')\n",
        "    print('Wrote lines {} to {} from {}'.format(startline, lineno, title))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki9f8GIN2uyn",
        "outputId": "f275a468-d017-4004-a7f9-77142169effe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote lines 0 to 2802 from Victorian songs\n",
            "Wrote lines 2802 to 7503 from Baldwin collection\n",
            "Wrote lines 7503 to 14069 from Swinburne collection\n",
            "Wrote lines 14069 to 14926 from Blake\n",
            "Wrote lines 14926 to 42441 from Bulchevys collection\n",
            "Wrote lines 42441 to 51789 from Palgrave-Pearse collection\n",
            "Wrote lines 51789 to 56298 from Knowles collection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the data\n",
        "with open('data/poetry/raw.txt', 'r') as rawfp,\\\n",
        "  open('data/poetry/input.txt', 'w') as infp,\\\n",
        "  open('data/poetry/output.txt', 'w') as outfp:\n",
        "\n",
        "    prev_line = ''\n",
        "    for curr_line in rawfp:\n",
        "        curr_line = curr_line.strip()\n",
        "        # poems break at empty lines, so this ensures we train only\n",
        "        # on lines of the same poem\n",
        "        if len(prev_line) > 0 and len(curr_line) > 0:\n",
        "            infp.write(prev_line + '\\n')\n",
        "            outfp.write(curr_line + '\\n')\n",
        "        prev_line = curr_line"
      ],
      "metadata": {
        "id": "5t-cxcFu5sGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 data/poetry/*.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40JS3xtH5s6q",
        "outputId": "1bc179cc-7fab-420e-c5ad-c76d59acc22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> data/poetry/input.txt <==\n",
            "i sat beside the streamlet\n",
            "i watched the water flow\n",
            "as we together watched it\n",
            "one little year ago\n",
            "the soft rain pattered on the leaves\n",
            "\n",
            "==> data/poetry/output.txt <==\n",
            "i watched the water flow\n",
            "as we together watched it\n",
            "one little year ago\n",
            "the soft rain pattered on the leaves\n",
            "the april grass was wet\n",
            "\n",
            "==> data/poetry/raw.txt <==\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the dataframe\n",
        "ind=0\n",
        "poem_stanzas=[]\n",
        "\n",
        "poem_lines=[]\n",
        "\n",
        "with open('data/poetry/raw.txt', 'r') as rawfp:\n",
        "  prev_line = ''\n",
        "  curr_stanza=\"\"\n",
        "  for curr_line in rawfp:\n",
        "    curr_line = curr_line.strip()\n",
        "    # poems break at empty lines, so this ensures we train only\n",
        "    # on lines of the same poem\n",
        "    if len(prev_line) > 0 and len(curr_line) > 0:\n",
        "      curr_stanza=curr_stanza+curr_line+\"\\n\"\n",
        "    else:\n",
        "      if len(curr_stanza) > 0:\n",
        "        poem_stanzas.append(curr_stanza)\n",
        "      curr_stanza=\"\"\n",
        "\n",
        "    prev_line = curr_line\n",
        "    # ind+=1\n",
        "\n",
        "    # if ind==100:\n",
        "    #   break"
      ],
      "metadata": {
        "id": "-Kf9KjUZOJK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pQZ9Dp7-SIus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stanza=pd.DataFrame({\"stanza_num\":[x for x in range(len(poem_stanzas))], \"stanza\":poem_stanzas})"
      ],
      "metadata": {
        "id": "-XgLmUd7QNDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stanza.to_csv(\"data/poetry/poem_stanza.csv\",index=False)"
      ],
      "metadata": {
        "id": "TUlZywV7SapC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir poetry/trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y22W0Lk52ur",
        "outputId": "24f6e83c-56f8-436e-d06e-3013c8945036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘poetry/trainer’: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XiA1Y4H2P-Qt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}